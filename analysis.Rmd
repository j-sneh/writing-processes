---
title: "STAT 432 Final Project Fall 23"
author: "Abhi Thanvi (athanvi2), Jonathan Sneh (jsneh2)"
date: "2023-12-04"
output: 
  pdf_document: default
toc: yes
header-includes: 
- \usepackage{xcolor}
- \definecolor{salmon}{RGB}{250,150,114}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(2023)
```

```{r echo=FALSE, message=FALSE}
if (!require("stats")) install.packages("stats")
library(stats)
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)
if (!require("tidyr")) install.packages("tidyr")
library(tidyr)
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

if (!require("knitr")) install.packages("knitr")
library(knitr)
if (!require("kableExtra")) install.packages("kableExtra")
library(kableExtra)
if (!require("reshape2")) install.packages("reshape2")
library(reshape2)

if (!require("cluster")) install.packages("cluster")
library(cluster)
if (!require("umap")) install.packages("umap")
library(umap)
if (!require("RColorBrewer")) install.packages("RColorBrewer")
library(RColorBrewer)
library(gridExtra)
if (!require("VGAM")) install.packages("VGAM")
library(VGAM)
if (!require("caret")) install.packages("caret")
library(caret)
if (!require("xgboost")) install.packages("xgboost")
library(xgboost)
```


\newpage
# \textcolor{salmon}{Project Overview} \vspace{4mm}
We highly recommend everyone to checkout our \href{https://github.com/j-sneh/writing-processes}{\textcolor{blue!70!white}{Github Repository}}
for all the data cleaning, feature engineering, analysis, and other files! Many of our procedures are not included in the report, and the repo provides a behind-the-scenes access to that information! We understand it is important to understand each procedure but also reproduce results, therefore we have maintained a readable code-base for it! :)

## \textcolor{magenta!80!black}{Goals}

While this project is an assigned project for STAT 432 Fall 2023 (UIUC), our goal is to apply what we have learned in our class and generate not necessarily the "most accurate", but rather the most holistic solution on this unique problem. The topic we are dealing with \href{https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality}{\textcolor{blue!70!white}{Linking Writing Processes to Writing Quality}} where we explore data on typing behavior to predict essay quality between a score of 0-6 (inclusive) using many of the statistical learning techniques.
 
## \textcolor{magenta!80!black}{Approach} 
We divide our work into three main section. 

  - **Data Processing** $\rightarrow$ Clean, extract, and engineer features in python notebook. Important for our Supervised and Unsupervised learning portion.
  
  - **Unsupervised Learning** $\rightarrow$ Perform clustering algorithms on the cleansed data (and 80-20 split).
  We chose to do K-Means and Hierarchical Clustering algorithms as we wanted to explore what we learned in class. These unsupervised techniques allow us to find hidden patterns in our data and act as an outlet for advanced EDA.
  
  - **Regression/Classification Models** $\rightarrow$ It is in this section where we try to predict scores and aim to achieve our original goal. TBD

## \textcolor{magenta!80!black}{Unique Approaches/Techniques:} 
We generally tried to use as much as we could from our STAT 432 course materials as the source of knowlegde. There were moments where we did consult other topics such as pairing elbow method with Silhouette Plots (to determine how well the cluster fit), \textcolor{red}{**JONATHAN ADD YOUR STUFF HERE**...prolly the STAT 426 stuff?} :D

## \textcolor{magenta!80!black}{Conclusion:} TBD


\newpage
# \textcolor{salmon}{Literature Review}
We aim to understand our pursuit of understanding and addressing the problem statement, we embarked on a journey of reviewing existing submissions to gain insights and inspiration. This preliminary exploration aimed to familiarize ourselves with various approaches employed by others in the field.

## \textcolor{magenta!80!black}{Noteworthy Submissions}

Upon reviewing the top-performing submissions, we observed a prevalent trend—the use of topics beyond the scope of our class and a preference for Python implementation. Among these submissions, one that stood out was by Maok Yongsuk's \href{https://www.kaggle.com/code/yongsukprasertsuk/writing-processes-to-quality-0-584/notebook}{\textcolor{blue!70!white}{Noteboook}} which achieved the highest public score of 0.584.

## \textcolor{magenta!80!black}{Maok Yongsuk's Approach}

Maok's methodology deviated from the conventional as he introduced an essay constructor for tokenization. This innovative approach facilitated the extraction of meaningful features from textual data, enabling the model to capture intricate patterns within essays. Notably, he opted for Python's `LightGBM` function, a variant of the XGBoost framework employing a leaf-wise tree growth strategy. This strategic choice results in a more balanced and potentially shallower tree, enhancing efficiency on large datasets. Another key aspect that caught our attention was Maok's intelligent use of feature engineering. Inspired by others in the field, he incorporated a set of features commonly utilized by Kaggle community members. These features, identified through rigorous exploration and collaboration, added valuable information to his model.

## \textcolor{magenta!80!black}{Other Submission Approaches}

In addition to Maok Yongsuk's approach, several other submissions demonstrated the utilization of topics outside our coursework. Notably, deep learning-based solutions and the combination of neural networks (NN) with `LightGBM` were prevalent strategies. Deep learning-based solutions and the integration of neural networks (NN) with LightGBM offer the potential to enhance prediction accuracy and create adaptive models. However, these approaches come with notable challenges. Firstly, they often incur increased computation costs, demanding substantial resources for training and potentially limiting their feasibility in resource-constrained projects. The inherent complexity of deep learning models poses interpretability challenges, hindering a clear understanding of their inner workings. 

All in all, It was a common thread among all submissions to engage in feature engineering that made sense during the exploratory data analysis (EDA) process, which is a practice we intend to adopt. Furthermore, there is a consideration of exploring gradient boosting techniques for our project, aligning with what we know/learned from class and the strategies employed by successful submissions.

## \textcolor{magenta!80!black}{Implications for Our Project}

The incorporation of `LightGBM` for predictions, the utilization of engineered features through tokenization (including those derived from the essay constructor), and the integration of additional features sourced from the Kaggle community collectively contributed to Maok's remarkable achievement with a highest public score of 0.584. In considering our own project, we find inspiration in these ideas from all submissions we reviewed, evaluating their relevance to our class material and potential applicability to our unique context. As we progress, we aim to integrate insights from Maok's and other Kaggle submission approaches into our methodology, adapting and refining these concepts to enhance the robustness of our own model.

## \textcolor{magenta!80!black}{Citations}

- Maok Yongsuk's Kaggle Submission: \href{https://www.kaggle.com/code/yongsukprasertsuk/writing-processes-to-quality-0-584}{\textcolor{blue!70!white}{Maok Yongsuk}}

- Other Submission: \href{https://www.kaggle.com/code/cody11null/lgbm-x2-nn}{\textcolor{blue!70!white}{Cody’s LGBM + NN}}

- Other Submission 2 (LGBM + NN): \href{https://www.kaggle.com/code/seoyunje/feature-1-lgb-nn-ridge}{\textcolor{blue!70!white}{Seoyunje’s LGBM + NN}}

- Research 1: \href{https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#:~:text=The%20silhouette%20plot%20displays%20a,like%20number%20of%20clusters%20visually.}{\textcolor{blue!70!white}{Research - Silhouette Plots}}

- Research 2: \href{https://towardsdatascience.com/implementing-and-interpreting-ordinal-logistic-regression-1ee699274cf5}{\textcolor{blue!70!white}{Ordinal Logistic Regression}}

# \textcolor{salmon}{Data Processing}

This section dives into the tasks performed for data processing. All the steps ensure the specifications of the projects were met, but some decisions were also made to ensure a more practical data to work with. To be considerate of the pages used for the Data Processing, we performed our Data Engineering steps in a `jupyter notebook` that you can view in our repo! 

## \textcolor{magenta!80!black}{Feature Engineering}

- **User ID** [`id`, `string`] — Unique IDs of each user.
  - We keep this to ensure tracking of user information for processing and analysis work.
- **Event ID** [`event_id`, `string`] — Incremental ID log of all events.
  - We keep this for processing steps, but remove it prior to analysis. The event IDs are useful as an ordinal feature of the log data.
- **Down Time / Up Time** [`down_time` / `up_time`, `integer`] — Time of event on down and up strokes of key or button, in seconds.
  - We summarize these features as an array of summary statistics; min, max, mean, median, and standard deviation. Measures of interest are max (i.e., how long a paper is written) and mean/median (i.e., when the center of most activity is).
- **Action Time** [`action_time`, `integer`] — Difference of time between down time and up time of event, i.e., duration of action in seconds. 
  - Similarly, we summarize this feature as min, max, mean, median, and std. This gives insight into "major" consecutive actions, hesitancy, or other special behaviors.
- **Activity** [`activity`, `string`] — Actions to edit or modify the text (input, remove/cut, nonproduction, etc.)
  - We compute the proportions of each of these activities. All of the cursor "Move From" events are mapped to one category called "Move From". We choose proportions over count to avoid undue influence of essays that take longer to write.
- **Down Event**
  - We compute the proportions of each of the activities. The events were pooled into four categories: alphanumeric, special_characters, control_keys, and unknown.
- **Up event**
  - Since these are the same events as down events, we ignore this feature.
- **Text Change**
  - We process and cluster these values into identified patterns of changes: many characters (at least 2 alphanumeric), at least one character (exactly one alphanumeric), non-zero characters (no alphanumeric). We also identified "transition" groups of "X to Y" for each of "many", "single", "none" (e.g., "many" to "many", "many" to "single", "many" to "none", etc.). There was also a "no change" group. We created one additional group to represent the sum of all "transition" events because they coincided exclusively with "replacement" activities.
- **Cursor Position**
  - We computed an artificial array of cursor positions with the assumption that the text was streamed with no edits corresponding to what text changes there are (i.e., non-decreasing and doesn't change if "no change" is observed in text change feature). Then we compute the MAE error metric between this stream version and the actual cursor positions to measure how much error exists between them. Greater errors imply more frequent and/or drastic changes.
- **Word Count**
  - We summarize these features as an array of summary statistics; min, max, mean, median, and standard deviation. We are primarily interested in the maximum measure as it indicates the length of the paper for each user.

## \textcolor{magenta!80!black}{Data Summary}

```{r load data, echo=FALSE}
df = read.csv("data/df_user_summaries_select.csv")
```

Here is a sample of the processed data.

```{r show data, echo=FALSE}
df_show = t(head(df))
colnames(df_show) = df$id[1:6]
# Assuming df is your dataframe
df_show %>% kable("latex", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("striped", "HOLD_position", "scale_down"))
```


## \textcolor{magenta!80!black}{Correlation Summary}

```{r plots, echo=FALSE, message=FALSE, fig.dim=c(6,6)}
df$mae_cursor_position_norm = df$mae_cursor_position / df$word_count_max
# Assuming 'data' is your dataframe
numeric_features <- df %>% select_if(is.numeric)
# Compute the correlation matrix
cor_matrix <- cor(numeric_features, use = "complete.obs")
# Reshape the data
melted_cor_matrix <- melt(cor_matrix)
# Create the heatmap
ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, 
                       limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_blank()) +coord_fixed()
```

We observe some pairs of features that show signs of multicolinearity. 

- The word count metrics are highly correlated as expected, we could reasonably choose the maximum measure to use.

- Some features form parallel or perpendicular colinearity. 
  - activity_input and activity_nonproduction (negative)
  - activity_input and down_event_alphanumeric (positive)
  - activity_input and down_event_control_keys (negative)
  - activity_input and text_change_nochange (negative)
  
- Importantly, we're interested in what's correlated with the user score feature
  - word count measures have a positive correlation with score, suggesting an association between longer essays and higher scores
  - Error rate of cursor positions against a "streamed" output also shows a positive correlation with score - i.e., essays written with less frequent or extreme edits is somewhat associated with higher scores.
  - Note: a positive correlation is also found between error rate of cursor positions with max word count, suggesting further that longer essays are associated with higher deviation from a "streamed" output. This suggests the possibility that interpretation of "streamed" deviation is influenced by the paper length (i.e., longer papers support possibility of edits being made "further away" from the current "streamed" position, thus increasing the error rate). When we normalized the error rate by the paper size, we see that the correlation between the normalized error rate and the paper score is nearly zero. So, this feature is likely irrelevant for analysis.

# \textcolor{salmon}{Unsupervised Learning Algorithms}

This section dives into the tasks performed for the unsupervised learning algorithms. We mainly focused on K-Means and Hierarchical Clustering. 

```{r TrainTestSplit, include=FALSE}
# test train split
test_ids = sample(1:nrow(df), as.integer(0.2 * nrow(df)))
data = as.data.frame(scale(df[, -(1:2)]))
data = cbind(score=df$score, data)
train = data[-test_ids, ]
test = data[test_ids, ]
```

## \textcolor{magenta!80!black}{K-Means Algorithm}

The first question we want to asks ourselves is how many clusters do we want? While we did play around with cluster numbers, we chose to use the elbow-method to determine our count for clusters. 

```{r elbow, message=FALSE, echo=FALSE, warning=FALSE, fig.dim=c(5,3)}
# Range of k values to try
num_clusters <- 1:14

# Initialize a vector to store average within-cluster sum of squares (WCSS)
avg_wcss_list <- numeric(length(num_clusters))

# Iterate over different values of k
for (k in num_clusters) {
  sub_wcss_list <- numeric(3) # For storing WCSS of each trial
  for (i in 1:3) {
    set.seed(i) # Setting seed for reproducibility
    kmeans_result <- kmeans(train, centers=k, nstart=25, iter.max = 50)
    sub_wcss_list[i] <- kmeans_result$tot.withinss
  }
  avg_wcss_list[k] <- mean(sub_wcss_list)
}

# Plotting the elbow plot with improved colors and labels
ggplot(data.frame(Clusters=num_clusters, WCSS=avg_wcss_list), 
       aes(x=Clusters, y=WCSS)) +
  geom_line(color = "steelblue", size = 1.5) +  # Simpler blue color
  geom_point(color = "darkorange", size = 3) +  # Orange points
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "gray", linetype = "dashed"), 
        axis.text = element_text(color = "black"),  
        plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 12)) +
  ggtitle("Optimal Number of Clusters using K-Means") +
  xlab("Number of Clusters") +
  ylab("Average WCSS")
```

Although slightly hard to see the elbow, we selected `k=5` clusters to fit our k-means model as it seemed closest to the elbow of the plot we have above.

```{r kmeans, include=FALSE}
set.seed(100)
# Perform K-means clustering
# Here, we are specifying 5 clusters, but you can change this number
result <- kmeans(train, centers=5)
```

```{r kmeansgraph, echo=FALSE, fig.dim=c(6,3)}
# Plotting the K-means clustering results
ggplot(data.frame(train), aes(x=score, y=word_count_max)) +
  geom_point(aes(color=factor(result$cluster)), 
             size = 3, alpha = 0.8) +
  scale_color_manual(name="Cluster", 
                     values = c("darkorange", "forestgreen", 
                                "red", "purple", "steelblue")) +
  theme_minimal() +
  theme(legend.position="bottom") +
  ggtitle("K-means Clustering (k=5)") +
  xlab("Score") +
  ylab("Word Count Max")
```

It seems like the clusters aren't clearly distinct and have overlapping present from the above graph. 

Therefore, we used silhouette scores to evaluate how well the clustering structure fits in terms of similarity, i.e., more higher scores in a cluster imply greater similarity of points to its own cluster and poorer similarity to other clusters. 

```{r silhouette, echo=FALSE, fig.dim=c(5,4)}
# Compute silhouette information
silhouette_info <- silhouette(result$cluster, dist(train))

# Define a custom color palette for silhouette plot
custom_palette <- c("#377eb8", "#ff7f00", "#4daf4a", "#f781bf", "#a65628")

plot(silhouette_info, col=custom_palette, border=NA, main="Silhouette Plot",
     cex.names=0.8, cex.axis=0.8, cex.main=1, cex.lab=0.9, density=70)
```

The average silhouette score of 0.17 suggests that the clusters are somewhat favorably well separated and that the points within clusters aren't too dispersed. However, we can see negative score with cluster 3 which shows issues with cohesion and separation.

Let's see what going in more detail by projecting the data using `umap`. This should give us some more insights into the hidden patterns that are present within our data. 

```{r cluster_umpa, echo=FALSE, fig.dim=c(6,4)}
umap_result <- umap(train, n_components = 2)

umap_data <- as.data.frame(umap_result$layout)
colnames(umap_data) <- c("UMAP1", "UMAP2")

clusters <- as.factor(result$cluster)
umap_data$Cluster <- clusters

# Choose a palette
palette <- brewer.pal(n = 5, name = "Set1") 

# Plotting the UMAP projection with enhancements
ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = Cluster)) +
  geom_point(alpha = 0.6, size = 2) +  # Adjust transparency and point size
  scale_color_manual(values = palette) +
  theme_minimal() +
  theme(legend.position="right") +
  ggtitle("UMAP Projection of the Data (Clusters)") +
  xlab("UMAP Dimension 1") +
  ylab("UMAP Dimension 2")
```

```{r umapscore, echo=FALSE, fig.dim=c(6,4)}
scores <- as.ordered(as.data.frame(train)$score)
umap_data$Score <- scores

# Choose a palette for score levels
score_palette <- rainbow(12)

# Plotting the UMAP projection with score coloring and enhancements
ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = Score)) +
  geom_point(alpha = 0.6, size = 2) +
  scale_color_manual(values = score_palette, name = "Score Level") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("UMAP Projection of the Data (Score)") +
  xlab("UMAP Dimension 1") +
  ylab("UMAP Dimension 2")
```

We see that the clusters might not be very well separated and aren't globular, so it is reasonable to conclude that `K-Means` algorithm may struggle with this data in general and gives us key insight on the underlying structure of the data not being nicely seperable. This suggests further specialized data engineering or other advanced techniques might be helpful.

Investigation of the score distribution confirms and suggests that the underlying clustering structure of the data does not closely align with the structure of scores. Some trends are still present such as cluster 1 tends to capture those who score 3 to 6. (very spread apart). Similar trends can been seen using the graph above.

While there are some relationships, it would appear that the dispersions of scores between groups tend to overlap heavily and are not well separate to segment the groups in a very meaningful way. However clusters 1 and 2 vs. clusters 3, 4, and 5 seem to show some disparity, but this isn't super useful as we care about clusters that are well-seperated amongst one another also. 

```{r cont_table, include=FALSE}
# Create a contingency table without normalization
contingency_table <- table(result$cluster, df$score[-test_ids])

# View the contingency table
print(contingency_table)
```
Let's use K-Means on testing data and see how the cluster assignments look like
```{r testcluster, include=FALSE}
assign_cluster <- function(new_data, centers, distance_method = "euclidean") {
  # Calculate distances from each new data point to each cluster center
  distances <- as.matrix(dist(rbind(centers, new_data), method = distance_method))
  distances <- distances[(nrow(centers) + 1):nrow(distances), 1:nrow(centers)]
  
  # Assign each new data point to the nearest cluster
  max.col(-distances)
}
new_clusters <- assign_cluster(test, result$centers)
```

```{r plotcluster, echo=FALSE}
# Assuming 'new_clusters' and 'df' are available
acc_data <- as.matrix(
  t(table(new_clusters, df[test_ids, ]$score)))

# Create a list to store individual plots
plot_list <- list()

for (i in 1:5) {
  prop_plot <- ggplot(
    data.frame(
      Index = rownames(acc_data), Proportion = acc_data[, i]
    ),
    aes(x = Index, y = Proportion, fill = factor(Index))
  ) +
    geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    labs(title = paste("Cluster", i)) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    )

  plot_list[[i]] <- prop_plot
}

# Combine plots into a single grid
grid_plot <- do.call(grid.arrange, c(plot_list, ncol = 2))  # Arrange in a grid
```
When considering the underlying relation to scores, KMeans fails to achieve seperability between clusters. In other words, while the clusters within seem strong and cohesive, they tend to be overlapping with other clusters (i.e. scores are overlapping amongs clusters) making k-means an overall bad fit for this kind of non-seperable data. 

## \textcolor{magenta!80!black}{Hierarchial Clustering}
We select 7 clusters because there are 7 integer scores (after rounding half scores). The clustering structure in the data seems to support this number of clusters.

```{r, include=FALSE}
distance_matrix <- dist(train, method = "euclidean")

# Complete Linkage
hc_complete <- hclust(distance_matrix, method = "complete")

# Single Linkage
hc_single <- hclust(distance_matrix, method = "single")

# Average Linkage
hc_average <- hclust(distance_matrix, method = "average")

# Plotting dendrograms
par(mfrow = c(1, 3))
plot(hc_complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.9)
plot(hc_single, main = "Single Linkage", xlab = "", sub = "", cex = 0.9)
plot(hc_average, main = "Average Linkage", xlab = "", sub = "", cex = 0.9)
```

```{r, echo=FALSE, fig.dim=c(5,4)}
# Complete Linkage
#hc_complete <- hclust(distance_matrix, method = "complete")
k <- 7 # Number of clusters
plot(hc_complete, main = "Hierarchical Clustering with Complete Linkage", 
     xlab = "", sub = "", cex = 0.9)
rect.hclust(hc_complete, k = k, border = "red")  # You can change the border color if you like
```



```{r, echo=FALSE, fig.dim=c(5,3)}
# clustering structure in UMAP
clusters <- as.factor(cutree(hc_complete, k = k))
umap_data$ClusterHC <- clusters

# Choose a palette
palette <- brewer.pal(n = k, name = "Set1")  # Adjust 'name' as needed

ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = ClusterHC)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = palette) +
  theme_minimal() +
  ggtitle("UMAP Projection of the Data")
```


```{r, include=FALSE}
#Our Training classification Table
t(table(umap_data$ClusterHC, df[-test_ids, ]$score))
```

```{r, include=FALSE}
# Let's predict on Test Data and see how does the clusters look like with the 
# hierarchial clustering algorithm.

# Function to calculate centroids of clusters
calculate_centroids <- function(data, clusters) {
  aggregate(data, by=list(cluster=clusters), FUN=mean)
}

# Function to predict the cluster of new data
predict_cluster <- function(new_data, train_data, clusters) {
  centroids <- calculate_centroids(train_data, clusters)
  # Remove the cluster column
  centroids <- centroids[, -1]
  
  # Function to find nearest centroid
  find_nearest_centroid <- function(point, centroids) {
    dists <- apply(centroids, 1, function(centroid) dist(rbind(centroid, point)))
    which.min(dists)
  }
  
  apply(new_data, 1, find_nearest_centroid, centroids = centroids)
}

new_clusters <- predict_cluster(test, train, clusters)
```

```{r, include=FALSE}
acc_data = as.matrix(
  t(table(new_clusters, as.integer(df[test_ids, ]$score))))

acc_data
```
Most of our predictions are going into cluster 1. This seems like a bad algorithm too, there's lots of scores overlapping into 1 cluster! In general, since the data is not seperable and not globular in structure it is hard for clustering algorithms to perform well. A PCA might be useful to reduce dimensionality and perform clustering, but we decided to maintain interpretability of our data and not use PCA in our supervised learning analysis.

# \textcolor{salmon}{Supervised Learning Algorithms}

## \textcolor{magenta!80!black}{Proportional Odds Model (GLM)}

Since the score is an ordinal variable, we can use a proportional odds model—a type of linear model—to predict the score. We'll use the `VGAM` package to fit the model and use the `step4` function to do a stepwise selection to find the best model. `VGAM` is a package that allows for fitting of a multinomial/propodds (vector based) linear model. It assumes cumulative probabilities and keeps the same $\beta$ for all categories, but allows for different intercepts. 

We'll use only non-collinear features (`score, word_count_max, down_event_special_character, mae_cursor_position, down_time_std, down_event_control_keys, text_change_not_q, activity_input`) and do a subset selection. We'll also convert `score` to an ordered factor to ensure that the model knows that it is an ordinal variable.

```{r, include=FALSE}
train.supervised <- train
train.supervised$score <- as.ordered(train.supervised$score)
train.supervised <- subset(train.supervised, select=c(score, word_count_max, down_event_special_character, mae_cursor_position, down_time_std, down_event_control_keys, text_change_not_q, activity_input))

test.supervised <- test
test.supervised$score <- as.ordered(test.supervised$score)
test.supervised <- subset(test.supervised, select=c(score, word_count_max, down_event_special_character, mae_cursor_position, down_time_std, down_event_control_keys, text_change_not_q, activity_input))

conv <- function(x) { 
  as.numeric(as.character(x)) 
}

labels <- levels(train.supervised$score)

score_from_factor <- function(x) {
  labels[x]
}

head(train.supervised)
```

```{r, results='hide'}
prop.wc <- vglm(score ~ word_count_max, data = train.supervised, family = propodds(reverse = F))

prop.upper <- vglm(score ~ ., data = train.supervised, family = propodds(reverse = F))

summary(prop.upper)
```

We can do a stepwise selection, starting from all variables, to find the best model. This will pick the model with the lowest AIC, which aims for better prediction error. 

```{r, results='hide', message=FALSE}
prop.step <- step4(prop.upper, scope = list(lower = prop.wc, upper = prop.upper), direction = "both")

summary(prop.step)
```

Looking at our selected model, we see that it drops `text_change_q`. Furthermore, we notice that the largest magnitude coefficient is `-1.61549` for `word_count_max`. For the propodds model we trained, if a coefficient is negative, it means the probability of falling into a lower category decreases as the predictor increases. This makes sense, since we saw that the word count was positively correlated with the score. `down_event_control_keys` and `activity_input` are negatively correlated with the score, and we see positive coefficients for them, which checks out.

Using our selected model, we can predict the probabilities of falling into each category and selecting the category with the highest probability as the predicted score. We can then compare the predicted scores to the actual scores to see how well our model did.

```{r, include=FALSE}
train.prop.probs <- predict(prop.step, newdata = train.supervised, type = "response")

get_score <- function(x) { labels[which.max(x)] }
# Column with maximum probability is the predicted score (label)
train.prop.pred_score <- apply(train.prop.probs, 1, get_score)
```
```{r}
# Confusion Matrix
prop.t1 <- (table(train.prop.pred_score, train.supervised$score))
# Accuracy
cat("Training Accuracy: ", mean(conv(train.prop.pred_score) == conv(train.supervised$score)))
# MAE
cat("Training MAE: ", mean(abs(conv(train.prop.pred_score) - conv(train.supervised$score))))
```

```{r, echo=FALSE}
test.prop.probs <- predict(prop.step, newdata = test.supervised, type = "response")

# Column with maximum probability is the predicted score (label)
test.prop.pred_score <- apply(test.prop.probs, 1, get_score)

# Confusion Matrix
prop.t2 <- (table(test.prop.pred_score, test.supervised$score))
# Accuracy
cat("Testing Accuracy: ", mean(conv(test.prop.pred_score) == conv(test.supervised$score)))
# MAE
cat("Testing MAE: ", mean(abs(conv(test.prop.pred_score) - conv(test.supervised$score))))
```

```{r, echo=FALSE}
# plot both confusion matrix side by side for comparison
# align the two tables left and right

# scale the table size
# par(mar = c(4, 4, .1, .1))
# plot the first table
prop.k1 <- kable(prop.t1, caption = "Propodds Training Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)

prop.k2 <- kable(prop.t2, caption = "Propodds Testing Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)

prop.k2
```

Our model does okay, especially given that there are 12 categories. It predicts the correct score only around 32% of the time on the training and 28.7% on the testing data. From the testing confusion matrix, we see that the model is not very good at predicting the lower and upper extremes. It predicts a lot of mid level scores (3 - 4.5), but is not good at differentiating between them. The same results are seen in the training confusion matrix (see Appendix for more). 

This is why we can also look at the MAE, which will give us a better idea of how far off the score predictions are on average, since it is a numerical measure.

On average, the model is off by around 0.55 points on the training and 0.59 points on the testing data. This is a pretty good result, meaning that, on average, the score is only a bit more than one level off.

## \textcolor{magenta!80!black}{K Nearest Neighbors}

During our data analysis/unsupervised learning, we saw that the clustering algorithm(s) did not do a great job of separating the data. This is likely because the data has a lot of overlap. To see if this holds during actual prediction, we can try to use a KNN model to predict the score.

We'll use the `caret` package to tune our `k` value through 10-fold cross validation. A larger `k` will introduce more bias in the model, and a smaller `k` will have a larger variance. Since there are lots of records, a large `k` value could be useful, without introducing too much bias—so we'll test a range of `k` values from 1 to 400. 

```{r, results='hide', echo = FALSE}
set.seed(432)
library(caret)

control <- trainControl(method = "cv", number = 10)
kgrid <- data.frame(k = seq(1, 400, 10))

knn.tuned <- train(score ~ ., data = train.supervised, 
method = "knn", tuneGrid = kgrid, trControl = control)

knn.tuned
```

From the model output, we saw that the best `k` value is 71, with a (cross-validated) accuracy of around 30.66%. We expect to see similar results checking with the entire training dataset. This is similar to our proportional odds model. 

```{r, echo=FALSE}
set.seed(432)
knn.train.pred <- predict(knn.tuned, newdata = train.supervised)
# Confusion Matrix
knn.k1 <- kable(table(knn.train.pred, train.supervised$score), caption = "KNN Training Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)
# Accuracy
cat("Accuracy: ", mean(conv(knn.train.pred) == conv(train.supervised$score)))
# MAE
cat("MAE: ", mean(abs(conv(knn.train.pred) - conv(train.supervised$score))))
```

Checking the full training data, we see that the accuracy is 34%! This is high, but we should be careful. The model could be overfitting to the training data. 

```{r, echo=FALSE, fig.dim=c(4,3)}
plot(knn.tuned)
```

From the plot, we see the accuracy is low for small `k` values and quickly rises. After that, the accuracy seems to slowly decrease, demonstrating an increased bias (bias-variance tradeoff).

Let's also see how the model does on the testing data.

```{r, echo=FALSE}
set.seed(432)
knn.test.pred <- predict(knn.tuned, newdata = test.supervised)
# Confusion Matrix
knn.k2 <- kable(table(knn.test.pred, test.supervised$score), caption = "KNN Testing Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)
# Accuracy
cat("Accuracy: ",mean(conv(knn.test.pred) == conv(test.supervised$score)))
# MAE
cat("MAE: ", mean(abs(conv(knn.test.pred) - conv(test.supervised$score))))

knn.k2
```

The model does a noticeably bit worse on the testing data, with an accuracy of around 24.5% (10 percentage points). This is likely an indicator that the model is overfitting to the training data and clusters/predictor spread is different between the training and testing data. This MAE is higher than the proportional odds model (0.57 for training, and 0.66 for testing). The KNN model only looks at euclidean distances. Our unsupervised learning showed that the clusters are overlapping, so it may have a hard time differentiating between scores and can be less accurate than the proportional odds model.

## \textcolor{magenta!80!black}{xgboost}

We saw from the literature review that gradient boosting was effective. So, we'll use xgboost with multi:softmax to predict the score.

We'll tune the `eta` (learning rate), as this is a critical hyperparameter of gradient boosting. For small values of `eta`, the model will take longer to converge, but will be more accurate. For large values of `eta`, the model will converge quickly, but will be less accurate.

We'll use the default `max_depth` (6) and an `nrounds` of 15, as we saw overfitting with larger `nrounds` (50), and will reduce the computational time. We'll pick the model with the lowest training error.

```{r, echo = FALSE}
set.seed(432)
library(xgboost)
eta_vals <- expand.grid(eta = c(0.01, 0.02, 0.03, 0.04, 0.05))
accuracy <- rep(NA, nrow(eta_vals))
for (i in 1:nrow(eta_vals)) {
  model.xgb <- xgboost(data = as.matrix(train.supervised[, -1]), 
  label = as.numeric(train.supervised$score) - 1, 
  nrounds = 15, objective = "multi:softmax", 
  num_class = 12, eta = eta_vals$eta[i], verbose = 0)

  predictions <- predict(model.xgb, as.matrix(train.supervised[, -1]))
  labeled_predictions <- sapply(predictions + 1, score_from_factor)
  accuracy[i] <- mean(conv(labeled_predictions) == conv(train.supervised$score))
}
```

```{r, echo=FALSE}
# eta_vals <- expand.grid(eta = c(0.01, 0.02, 0.03, 0.04, 0.05))
cat("Eta Vals: 0.01, 0.02, 0.03, 0.04, 0.05")
cat("Accuracy per Eta:", accuracy)
best_eta <- eta_vals[which.max(accuracy),]
cat("Best Eta: ",best_eta)
```

We saw that the best `eta` value is `0.05`, with a training accuracy of `0.6024279`. This accuracy is much larger than any other training accuracy we've seen. xgboost can very quickly converge to 100% accuracy on the training data, especially with large `nrounds` and `eta` values. This may be an indicator that the model is overfitting to the training data. We saw this when we used larger `eta` values, so we chose smaller values and we can see how it does on the testing data.

```{r, echo=FALSE}
set.seed(432)
model.xgb <- xgboost(data = as.matrix(train.supervised[, -1]), 
label = as.numeric(train.supervised$score) - 1, nrounds = 15, 
objective = "multi:softmax", 
num_class = 12, eta = best_eta, verbose = 0)

train.predictions <- predict(model.xgb, as.matrix(train.supervised[, -1]))
train.labeled_predictions <- sapply(predictions + 1, score_from_factor)

# confusion matrix
xgb.k1 <- kable(table(train.labeled_predictions, train.supervised$score), caption = "XGBoost Training Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)
# train accuracy
cat("Train Accuracy: ", mean(conv(train.labeled_predictions) == conv(train.supervised$score)))
# train MAE
cat("Train MAE: ", mean(abs(conv(labeled_predictions) - conv(train.supervised$score))))
```

We observe a very high training accuracy, for reasons we explained previously. See appendix for the training confusion matrices.

```{r, echo=FALSE}
set.seed(432)
test.predictions <- predict(model.xgb, as.matrix(test.supervised[, -1]))
test.labeled_predictions <- sapply(test.predictions + 1, score_from_factor)

# confusion matrix
xgb.k2 <- kable(table(test.labeled_predictions, test.supervised$score), caption = "XGBoost Testing Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)

# # test accuracy
cat("Test Accuracy: ", mean(conv(test.labeled_predictions) == conv(test.supervised$score)))

# # test MAE
cat("Test MAE: ", mean(abs(conv(test.labeled_predictions) - conv(test.supervised$score))))

xgb.k2
```

Here, we get a testing accuracy of around 30%, which is still lower than the training data (as expected). This is the best test accuracy we've seen so far, as well as an MAE on the lower end (0.61). This makes sense, as gradient boosting is a powerful technique that can capture complex relationships between variables — not just linear relationships or clusters based on euclidean distance. However, all three models have similar accuracy and MAEs for the testing data, so it's hard to definitively say which is the best model—especially when there's randomness involved in the training process.

# \textcolor{salmon}{Appendix}

```{r, echo=FALSE}
prop.k1

knn.k1

xgb.k1
```