---
title: "STAT 432 Final Project Fall 23"
author: "Abhi Thanvi (athanvi2), Jonathan Sneh (jsneh2)"
date: "2023-12-04"
output: 
  pdf_document: default
toc: yes
header-includes: 
- \usepackage{xcolor}
- \definecolor{salmon}{RGB}{250,150,114}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(2023)
```

```{r echo=FALSE, message=FALSE}
if (!require("stats")) install.packages("stats")
library(stats)
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)
if (!require("tidyr")) install.packages("tidyr")
library(tidyr)
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

if (!require("knitr")) install.packages("knitr")
library(knitr)
if (!require("kableExtra")) install.packages("kableExtra")
library(kableExtra)
if (!require("reshape2")) install.packages("reshape2")
library(reshape2)

if (!require("cluster")) install.packages("cluster")
library(cluster)
if (!require("umap")) install.packages("umap")
library(umap)
if (!require("RColorBrewer")) install.packages("RColorBrewer")
library(RColorBrewer)
library(gridExtra)
if (!require("VGAM")) install.packages("VGAM")
library(VGAM)
if (!require("caret")) install.packages("caret")
library(caret)
if (!require("xgboost")) install.packages("xgboost")
library(xgboost)
```


\newpage
# \textcolor{salmon}{Project Overview} \vspace{2mm}
We highly recommend everyone to checkout our \href{https://github.com/j-sneh/writing-processes}{\textcolor{blue!70!white}{Github Repository}}
for all the data cleaning, feature engineering, analysis, and other files! Many of our procedures are not included in the report, and the repo provides a behind-the-scenes access to that information! We understand it is important to understand each procedure but also reproduce results, therefore we have maintained a readable code-base for it! :)

## \textcolor{magenta!80!black}{Goals}

While this is an assigned project for STAT 432 Fall 2023 (UIUC), our goal was to apply what we have learned in our class and generate not necessarily the "most accurate", but rather the most holistic solution on this unique problem. The topic we are dealing with is  \href{https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality}{\textcolor{blue!70!white}{Linking Writing Processes to Writing Quality}} where we explore typing behavior to predict essay quality between a score of 0-6 (inclusive).
 
## \textcolor{magenta!80!black}{Approach} 
We divided our work into three main section. 

  - **Data Processing** $\rightarrow$ Cleaned, extracted, and engineered features in python notebook. Important for our Supervised and Unsupervised learning portion.
  
  - **Unsupervised Learning** $\rightarrow$ Performed clustering algorithms on the cleansed data (and 80-20 split).
  We chose to do K-Means and Hierarchical Clustering algorithms (paired with UMAP) as we wanted to explore what we learned in class. These unsupervised techniques allowed us to find hidden patterns in our data and act as an outlet for advanced EDA.
  
  - **Regression/Classification Models** $\rightarrow$  Predicted/classifued scores and attempted to achieve our original goal.To be specific, we performed `KNN` and `XGBoost` from what we learned in our class, but also performed `Proportional Odds Model (GLM)` which is something we learned in STAT 426. 

## \textcolor{magenta!80!black}{Unique Approaches/Techniques:} 
We generally used as much knowledge from STAT 432 as we could, but there were moments where we did consult other topics such as pairing elbow method with Silhouette Plots (to determine how well the cluster fit), and GLM model from STAT 426. We have cited sources we consulted including other Kaggle notebooks. :D

## \textcolor{magenta!80!black}{Conclusion:} 
We concluded this project with successful implementation of all the models we wanted to on the data. Unfortunately, the data is very complicated to work with for our basic modelling techniques to work super accurately! The reason is the data was not nicely separable and globular in nature which made it hard for the models to classify the user essays into accurate scores. Our predictions accuracy were better than we expected, but still not super high. On average, we were about `+/- 0.6` from the actual prediction score which isn't bad given our algorithms and features. There was a lot of overlap/confusion for the model predictions around the range of 3.5-4.5 due to lot of essays being in that region causing overlap between clusters and predicted scores. Based on that, research and experience, we highly recommend spending more efforts on feature engineering using Unsupervised/Supervised Learning techniques and advanced algorithms such as tokenization, Deep Learning (DL)/Neural Network (NN) architecture, etc. to get more confident and accurate scores for model predictions. 


\newpage
# \textcolor{salmon}{Literature Review}
Before we addressed the problem statement, we chose to go on a research journey of reviewing existing submissions to gain insights and inspiration from. This preliminary exploration greatly helped us, specially for feautre engineering, as we aimed to familiarize ourselves with various approaches employed by others in the field.

## \textcolor{magenta!80!black}{Noteworthy Submissions}

Upon reviewing the top-performing submissions, we observed a prevalent trend. The use of topics beyond the scope of our class and a preference for Python implementation. Among these submissions, one that stood out was by Maok Yongsuk's \href{https://www.kaggle.com/code/yongsukprasertsuk/writing-processes-to-quality-0-584/notebook}{\textcolor{blue!70!white}{Noteboook}} which achieved the highest public score of 0.584.

## \textcolor{magenta!80!black}{Maok Yongsuk's Approach}

Maok's methodology deviated from the conventional ones as he introduced an essay constructor for tokenization. This innovative approach facilitated the extraction of meaningful features from textual data, enabling the model to capture intricate patterns within essays. Notably, he opted for Python's `LightGBM` function, a variant of the XGBoost framework employing a leaf-wise tree growth strategy. This strategic choice resulted in a more balanced and potentially shallower tree, enhancing efficiency on large datasets. Another key aspect that caught our attention was Maok's intelligent use of feature engineering. He was by inspired by others and incorporated a set of features commonly utilized by Kaggle community members. These features were identified through rigorous exploration and collaboration by the community members, therefore adding valuable information to his model.

## \textcolor{magenta!80!black}{Other Submission Approaches}

In addition to Maok Yongsuk's approach, several other submissions demonstrated the utilization of topics outside our coursework. Notably, deep learning-based solutions and the combination of neural networks (NN) paired with `LightGBM` were prevalent strategies. Deep learning-based solutions and the integration of neural networks (NN) with LightGBM clearly enhanced prediction accuracy and created adaptive models for the submissions. However, these approaches came with notable challenges. Firstly, they often incur increased computation costs, demanding substantial resources for training and potentially limiting their feasibility in time and resource-constrained projects like ours. The obvious complexity of deep learning models posed interpretability challenges (for us and others) which hinders a clear understanding of their work's inner working. 

All in all, It was a common thread among all submissions to engage in feature engineering that made sense during the exploratory data analysis (EDA) process, which is a practice we adopted. Furthermore, there is a consideration of exploring gradient boosting techniques for our project, aligning with what we know/learned from class and the strategies employed by successful submissions.

## \textcolor{magenta!80!black}{Implications for Our Project}

The incorporation of `LightGBM` for predictions, the utilization of engineered features through tokenization (including those derived from the essay constructor), and the integration of additional features sourced from the Kaggle community collectively contributed to Maok's remarkable achievement with a highest public score of 0.584. In considering our own project, we drew inspiration from all submissions we reviewed and their ideas, evaluating their relevance to our class material and potential applicability to our unique context. As we progress, we aim to integrate insights from Maok's and other Kaggle submission approaches into our methodology, such as Feature Engineering and XGBoost to enhance the robustness of our own model. 

## \textcolor{magenta!80!black}{Citations}

- Maok Yongsuk's Kaggle Submission: \href{https://www.kaggle.com/code/yongsukprasertsuk/writing-processes-to-quality-0-584}{\textcolor{blue!70!white}{Maok Yongsuk}}

- Other Submission: \href{https://www.kaggle.com/code/cody11null/lgbm-x2-nn}{\textcolor{blue!70!white}{Cody’s LGBM + NN}}

- Other Submission 2 (LGBM + NN): \href{https://www.kaggle.com/code/seoyunje/feature-1-lgb-nn-ridge}{\textcolor{blue!70!white}{Seoyunje’s LGBM + NN}}

- Research 1: \href{https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#:~:text=The%20silhouette%20plot%20displays%20a,like%20number%20of%20clusters%20visually.}{\textcolor{blue!70!white}{Research - Silhouette Plots}}

- Research 2: \href{https://towardsdatascience.com/implementing-and-interpreting-ordinal-logistic-regression-1ee699274cf5}{\textcolor{blue!70!white}{Ordinal Logistic Regression}}

# \textcolor{salmon}{Data Processing}

This section dives into the tasks performed for data processing. All the steps ensure the specifications of the projects were met, but some decisions were also made to ensure a more practical data to work with. To be considerate of the pages used for the Data Processing, we performed our Data Engineering steps in a `jupyter notebook` that you can view in our repo! 

## \textcolor{magenta!80!black}{Feature Engineering}

- **User ID** [`id`, `string`] — Unique IDs of each user.
  - We keep this to ensure tracking of user information for processing and analysis work.
- **Event ID** [`event_id`, `string`] — Incremental ID log of all events.
  - We keep this for processing steps, but remove it prior to analysis. The event IDs are useful as an ordinal feature of the log data.
- **Down Time / Up Time** [`down_time` / `up_time`, `integer`] — Time of event on down and up strokes of key or button, in seconds.
  - We summarize these features as an array of summary statistics; min, max, mean, median, and standard deviation. Measures of interest are max (i.e., how long a paper is written) and mean/median (i.e., when the center of most activity is).
- **Action Time** [`action_time`, `integer`] — Difference of time between down time and up time of event, i.e., duration of action in seconds. 
  - Similarly, we summarize this feature as min, max, mean, median, and std. This gives insight into "major" consecutive actions, hesitancy, or other special behaviors.
- **Activity** [`activity`, `string`] — Actions to edit or modify the text (input, remove/cut, nonproduction, etc.)
  - We compute the proportions of each of these activities. All of the cursor "Move From" events are mapped to one category called "Move From". We choose proportions over count to avoid undue influence of essays that take longer to write.
- **Down Event**
  - We compute the proportions of each of the activities. The events were pooled into four categories: alphanumeric, special_characters, control_keys, and unknown.
- **Up event**
  - Since these are the same events as down events, we ignore this feature.
- **Text Change**
  - We process and cluster these values into identified patterns of changes: many characters (at least 2 alphanumeric), at least one character (exactly one alphanumeric), non-zero characters (no alphanumeric). We also identified "transition" groups of "X to Y" for each of "many", "single", "none" (e.g., "many" to "many", "many" to "single", "many" to "none", etc.). There was also a "no change" group. We created one additional group to represent the sum of all "transition" events because they coincided exclusively with "replacement" activities.
- **Cursor Position**
  - We computed an artificial array of cursor positions with the assumption that the text was streamed with no edits corresponding to what text changes there are (i.e., non-decreasing and doesn't change if "no change" is observed in text change feature). Then we compute the MAE error metric between this stream version and the actual cursor positions to measure how much error exists between them. Greater errors imply more frequent and/or drastic changes.
- **Word Count**
  - We summarize these features as an array of summary statistics; min, max, mean, median, and standard deviation. We are primarily interested in the maximum measure as it indicates the length of the paper for each user.

## \textcolor{magenta!80!black}{Data Summary}

```{r load data, echo=FALSE}
df = read.csv("data/df_user_summaries_select.csv")
```

Here is a sample of the processed data.

```{r show data, echo=FALSE}
df_show = t(head(df))
colnames(df_show) = df$id[1:6]
# Assuming df is your dataframe
df_show %>% kable("latex", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("striped", "HOLD_position", "scale_down"))
```

\newpage
## \textcolor{magenta!80!black}{Correlation Summary}
We observed some pairs of features that showed signs of multicolinearity. 

```{r plots, echo=FALSE, message=FALSE, fig.dim=c(7,4)}
df$mae_cursor_position_norm = df$mae_cursor_position / df$word_count_max
# Assuming 'data' is your dataframe
numeric_features <- df %>% select_if(is.numeric)
# Compute the correlation matrix
cor_matrix <- cor(numeric_features, use = "complete.obs")
# Reshape the data
melted_cor_matrix <- melt(cor_matrix)
# Create the heatmap
ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, 
                       limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6),
        axis.text.y =  element_text(hjust = 1, size = 6),
        axis.title = element_blank()) +coord_fixed()
```

Some features formed parallel or perpendicular colinearity. Examples: `activity_input` and `activity_nonproduction` (negative); `activity_input` and `down_event_alphanumeric` (positive); `activity_input` and `down_event_control_keys` (negative); `activity_input` and `text_change_nochange` (negative).
  
We were more interested in what's correlated with the user score feature: `word count` measures have a positive correlation with score, suggesting an **association between longer essays and higher scores**. `Error rate of cursor positions` against a "streamed" output also shows a positive correlation with score. In example, essays written with less frequent or extreme edits is somewhat associated with higher scores. **Please also note:** A positive correlation was also found between error rate of cursor positions with max word count, suggesting further that longer essays are associated with higher deviation from a "streamed" output. This suggests the possibility that interpretation of "streamed" deviation is influenced by the paper length (i.e., longer papers support possibility of edits being made "further away" from the current "streamed" position, thus increasing the error rate). When we normalized the error rate by the paper size, we see that the correlation between the normalized error rate and the paper score is nearly zero. So, this feature is likely irrelevant for analysis and was not considered later on.

# \textcolor{salmon}{Unsupervised Learning Algorithms}

This section dives into the tasks performed for the unsupervised learning algorithms. We mainly focused on K-Means and Hierarchical Clustering as a means to find hidden patterns within our data, in other words, to perform advanced EDA.

```{r TrainTestSplit, include=FALSE}
# test train split
test_ids = sample(1:nrow(df), as.integer(0.2 * nrow(df)))
data = as.data.frame(scale(df[, -(1:2)]))
data = cbind(score=df$score, data)
train = data[-test_ids, ]
test = data[test_ids, ]
```

## \textcolor{magenta!80!black}{K-Means Algorithm}

The section is where we drew insights from K-Means algorithm and it's relation to score feature. The first question we asked ourselves is how many clusters do we want? After experimenting, we chose to use the elbow-method to determine our count for clusters. 

```{r elbow, message=FALSE, echo=FALSE, warning=FALSE, fig.dim=c(6,2)}
# Range of k values to try
num_clusters <- 1:14

# Initialize a vector to store average within-cluster sum of squares (WCSS)
avg_wcss_list <- numeric(length(num_clusters))

# Iterate over different values of k
for (k in num_clusters) {
  sub_wcss_list <- numeric(3) # For storing WCSS of each trial
  for (i in 1:3) {
    set.seed(i) # Setting seed for reproducibility
    kmeans_result <- kmeans(train, centers=k, nstart=25, iter.max = 50)
    sub_wcss_list[i] <- kmeans_result$tot.withinss
  }
  avg_wcss_list[k] <- mean(sub_wcss_list)
}

# Plotting the elbow plot with improved colors and labels
ggplot(data.frame(Clusters=num_clusters, WCSS=avg_wcss_list), 
       aes(x=Clusters, y=WCSS)) +
  geom_line(color = "steelblue", size = 1.5) +  # Simpler blue color
  geom_point(color = "darkorange", size = 3) +  # Orange points
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "gray", linetype = "dashed"), 
        axis.text = element_text(color = "black"),  
        plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title = element_text(size = 12)) +
  ggtitle("Optimal Number of Clusters using K-Means") +
  xlab("Number of Clusters") +
  ylab("Average WCSS")
```

It is slightly hard to see the elbow, but we selected `k=5` clusters to fit our k-means model as it seemed closest to the elbow from the plot above.

```{r kmeans, include=FALSE}
set.seed(100)
# Perform K-means clustering
# Here, we are specifying 5 clusters, but you can change this number
result <- kmeans(train, centers=5)
```

```{r kmeansgraph, echo=FALSE, fig.dim=c(6,3)}
# Plotting the K-means clustering results
ggplot(data.frame(train), aes(x=score, y=word_count_max)) +
  geom_point(aes(color=factor(result$cluster)), 
             size = 3, alpha = 0.8) +
  scale_color_manual(name="Cluster", 
                     values = c("darkorange", "forestgreen", 
                                "red", "purple", "steelblue")) +
  theme_minimal() +
  theme(legend.position="bottom") +
  ggtitle("K-means Clustering (k=5)") +
  xlab("Score") +
  ylab("Word Count Max")
```

The clusters aren't clearly distinct and have overlapping present. Therefore, we used silhouette scores to evaluate how well the clustering structure fits in terms of similarity within our cluster. In other words, higher silhoutte score for a cluster suggest greater similarity of points within its own cluster and poorer similarity to other clusters which is what we consider to be a good seperable cluster. Referring to the `Silhouette Plot of 5 Clusters` (shown in the appendix), we saw that average silhouette score of 0.17 suggests that the clusters are somewhat favorably well separated and that the points within clusters aren't too dispersed. However, we can see negative score with cluster 3 which shows indicated issues of cohesion and separation. This kind of confirms our insights we drew from the `K-Means Clustering` plot initially. We dove into more detail by projecting the data using `umap`. This gave us some more insights into the hidden patterns that were present within our data. 

```{r silhouette, include=FALSE}
# In the Appendix, the graph was acting wonky (even with include=FALSE)
# So the code for the plot has also been moved to appendix :(
```

```{r cluster_umpa, echo=FALSE, fig.dim=c(6,3.5)}
umap_result <- umap(train, n_components = 2)

umap_data <- as.data.frame(umap_result$layout)
colnames(umap_data) <- c("UMAP1", "UMAP2")

clusters <- as.factor(result$cluster)
umap_data$Cluster <- clusters

# Choose a palette
palette <- brewer.pal(n = 5, name = "Set1") 

# Plotting the UMAP projection with enhancements
ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = Cluster)) +
  geom_point(alpha = 0.6, size = 2) +  # Adjust transparency and point size
  scale_color_manual(values = palette) +
  theme_minimal() +
  theme(legend.position="right") +
  ggtitle("UMAP Projection of the Data (Clusters)") +
  xlab("UMAP Dimension 1") +
  ylab("UMAP Dimension 2")
```

```{r umapscore, echo=FALSE, fig.dim=c(6,3.5)}
scores <- as.ordered(as.data.frame(train)$score)
umap_data$Score <- scores

# Choose a palette for score levels
score_palette <- rainbow(12)

# Plotting the UMAP projection with score coloring and enhancements
ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = Score)) +
  geom_point(alpha = 0.6, size = 2) +
  scale_color_manual(values = score_palette, name = "Score Level") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("UMAP Projection of the Data (Score)") +
  xlab("UMAP Dimension 1") +
  ylab("UMAP Dimension 2")
```
We saw that the clusters might not be very well separated and aren't globular from the UMAP, so it was reasonable to conclude that `K-Means` algorithm may struggle with this data in general. This also gave us key insight on the underlying structure of the data not being nicely separable. This suggested further specialized data engineering or other advanced techniques might be helpful, but we moved forward to explore more and progress our project forward.

```{r cont_table, include=FALSE}
# Create a contingency table without normalization
contingency_table <- table(result$cluster, df$score[-test_ids])

# View the contingency table
print(contingency_table)
```
So let's see how K-Means performs on testing data and see how the cluster assignments look like
```{r testcluster, include=FALSE}
assign_cluster <- function(new_data, centers, distance_method = "euclidean") {
  # Calculate distances from each new data point to each cluster center
  distances <- as.matrix(dist(rbind(centers, new_data), method = distance_method))
  distances <- distances[(nrow(centers) + 1):nrow(distances), 1:nrow(centers)]
  
  # Assign each new data point to the nearest cluster
  max.col(-distances)
}
new_clusters <- assign_cluster(test, result$centers)
```

```{r plotcluster, echo=FALSE, fig.dim=c(6,5)}
# Assuming 'new_clusters' and 'df' are available
acc_data <- as.matrix(
  t(table(new_clusters, df[test_ids, ]$score)))

# Convert acc_data matrix to Kable format
kmeans.test.ct <- kable(acc_data, caption = "K-Means Testing Contingecy Table") %>%
  kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), 
                font_size = 3)
# Create a list to store individual plots
plot_list <- list()

for (i in 1:5) {
  prop_plot <- ggplot(
    data.frame(
      Index = rownames(acc_data), Proportion = acc_data[, i]
    ),
    aes(x = Index, y = Proportion, fill = factor(Index))
  ) +
    geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    labs(title = paste("Cluster", i)) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    )

  plot_list[[i]] <- prop_plot
}

# Combine plots into a single grid
grid_plot <- do.call(grid.arrange, c(plot_list, ncol = 2))  # Arrange in a grid
```
While there are some relationships, it would appear that the dispersions of scores between groups tend to overlap heavily and are not well separate to segment the groups in a very meaningful way. However clusters 1 and 2 vs. clusters 3, 4, and 5 seem to show some disparity, but this isn't super useful as we care more about individual clusters being well-seperated.

When considering the underlying relation to scores, K-Means failed to achieve seperability between clusters. In other words, while the clusters within seem strong and cohesive, they tended to be overlapping with other clusters (i.e. scores are overlapping in different clusters) making k-means an overall bad fit for this kind of non-seperable data. As suggested before, specialized data engineering or other advanced techniques might be helpful.

## \textcolor{magenta!80!black}{Hierarchial Clustering}

For our hierarchial clustering, we performed with euclidean distance using `complete`, `single`, and  `average` linkage but chose `complete linkage` as it was the better resulting clustering linkage among the the three. Since our clustering structure in the datanicely supported 7 clusters (for the 7 integer scores after rounding half scores), we selected 7 clusters for the clustering. 

```{r, include=FALSE}
distance_matrix <- dist(train, method = "euclidean")

# Complete Linkage
hc_complete <- hclust(distance_matrix, method = "complete")

# Single Linkage
hc_single <- hclust(distance_matrix, method = "single")

# Average Linkage
hc_average <- hclust(distance_matrix, method = "average")

# Plotting dendrograms
par(mfrow = c(1, 3))
plot(hc_complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.9)
plot(hc_single, main = "Single Linkage", xlab = "", sub = "", cex = 0.9)
plot(hc_average, main = "Average Linkage", xlab = "", sub = "", cex = 0.9)
```

```{r, echo=FALSE, fig.dim=c(5,4)}
# Complete Linkage
#hc_complete <- hclust(distance_matrix, method = "complete")
k <- 7 # Number of clusters
plot(hc_complete, main = "Hierarchical Clustering with Complete Linkage", 
     xlab = "", sub = "", cex = 0.9)
rect.hclust(hc_complete, k = k, border = "red")  # You can change the border color if you like
```



```{r, echo=FALSE, fig.width=5, fig.height=3}
# clustering structure in UMAP
clusters <- as.factor(cutree(hc_complete, k = k))
umap_data$ClusterHC <- clusters

# Choose a palette
palette <- brewer.pal(n = k, name = "Set1")  # Adjust 'name' as needed

HC.umap.plot <- ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = ClusterHC)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = palette) +
  theme_minimal() +
  ggtitle("Hierarchial Clutering - UMAP Projection of the Data")

```


```{r, include=FALSE}
#Our Training classification Table
t(table(umap_data$ClusterHC, df[-test_ids, ]$score))
```

```{r, include=FALSE}
# Let's predict on Test Data and see how does the clusters look like with the 
# hierarchial clustering algorithm.

# Function to calculate centroids of clusters
calculate_centroids <- function(data, clusters) {
  aggregate(data, by=list(cluster=clusters), FUN=mean)
}

# Function to predict the cluster of new data
predict_cluster <- function(new_data, train_data, clusters) {
  centroids <- calculate_centroids(train_data, clusters)
  # Remove the cluster column
  centroids <- centroids[, -1]
  
  # Function to find nearest centroid
  find_nearest_centroid <- function(point, centroids) {
    dists <- apply(centroids, 1, function(centroid) dist(rbind(centroid, point)))
    which.min(dists)
  }
  
  apply(new_data, 1, find_nearest_centroid, centroids = centroids)
}

new_clusters <- predict_cluster(test, train, clusters)
```

```{r, include=FALSE}
acc_data = as.matrix(
  t(table(new_clusters, as.integer(df[test_ids, ]$score))))

acc_data
```
We noticed the issue of how the tree was being split (non-uniformly) and we wanted to explore more on what's going on. Therefore, we did another UMAP. If we refer to the `Hierarchial Clutering - UMAP Project of the Data` (shown in the appendix), we will clearly see that most of our predictions are going into cluster 1. This isn't ideal because this means lots of scores will be overlapping into 1 cluster! This confirms our previous insights we drew from K-Means section. 

Therefore, in general, we concludde that since the data is not seperable and not globular in structure it is hard for clustering algorithms to perform well. For future, a PCA might be useful to reduce dimensionality to perform clustering, but we made the decision to maintain interpretability of our data and not use PCA in our supervised learning analysis.

# \textcolor{salmon}{Supervised Learning Algorithms}

## \textcolor{magenta!80!black}{Proportional Odds Model (GLM)}

Since the score is an ordinal variable, we can use a proportional odds model—a type of linear model—to predict the score. We'll use the `VGAM` package to fit the model and use the `step4` function to do a stepwise selection to find the best model. `VGAM` is a package that allows for fitting of a multinomial/propodds (vector based) linear model. It assumes cumulative probabilities and keeps the same $\beta$ for all categories, but allows for different intercepts. 

We'll use only non-collinear features (`score, word_count_max, down_event_special_character, mae_cursor_position, down_time_std, down_event_control_keys, text_change_not_q, activity_input`) and do a subset selection. We'll also convert `score` to an ordered factor to ensure that the model knows that it is an ordinal variable.

```{r, include=FALSE}
train.supervised <- train
train.supervised$score <- as.ordered(train.supervised$score)
train.supervised <- subset(train.supervised, select=c(score, word_count_max, down_event_special_character, mae_cursor_position, down_time_std, down_event_control_keys, text_change_not_q, activity_input))

test.supervised <- test
test.supervised$score <- as.ordered(test.supervised$score)
test.supervised <- subset(test.supervised, select=c(score, word_count_max, down_event_special_character, mae_cursor_position, down_time_std, down_event_control_keys, text_change_not_q, activity_input))

conv <- function(x) { 
  as.numeric(as.character(x)) 
}

labels <- levels(train.supervised$score)

score_from_factor <- function(x) {
  labels[x]
}

head(train.supervised)
```

```{r, results='hide'}
prop.wc <- vglm(score ~ word_count_max, data = train.supervised, family = propodds(reverse = F))

prop.upper <- vglm(score ~ ., data = train.supervised, family = propodds(reverse = F))

summary(prop.upper)
```

We can do a stepwise selection, starting from all variables, to find the best model. This will pick the model with the lowest AIC, which aims for better prediction error. 

```{r, results='hide', message=FALSE}
prop.step <- step4(prop.upper, scope = list(lower = prop.wc, upper = prop.upper), direction = "both")

summary(prop.step)
```

Looking at our selected model, we see that it drops `text_change_q`. Furthermore, we notice that the largest magnitude coefficient is `-1.61549` for `word_count_max`. For the propodds model we trained, if a coefficient is negative, it means the probability of falling into a lower category decreases as the predictor increases. This makes sense, since we saw that the word count was positively correlated with the score. `down_event_control_keys` and `activity_input` are negatively correlated with the score, and we see positive coefficients for them, which checks out.

Using our selected model, we can predict the probabilities of falling into each category and selecting the category with the highest probability as the predicted score. We can then compare the predicted scores to the actual scores to see how well our model did.

```{r, include=FALSE}
train.prop.probs <- predict(prop.step, newdata = train.supervised, type = "response")

get_score <- function(x) { labels[which.max(x)] }
# Column with maximum probability is the predicted score (label)
train.prop.pred_score <- apply(train.prop.probs, 1, get_score)
```
```{r}
# Confusion Matrix
prop.t1 <- (table(train.prop.pred_score, train.supervised$score))
# Accuracy and MAE
cat("Training Accuracy: ", 
    mean(conv(train.prop.pred_score) == conv(train.supervised$score)), 
    " Training MAE: ", 
    mean(abs(conv(train.prop.pred_score) - conv(train.supervised$score))))
```

```{r, echo=FALSE}
test.prop.probs <- predict(prop.step, newdata = test.supervised, type = "response")

# Column with maximum probability is the predicted score (label)
test.prop.pred_score <- apply(test.prop.probs, 1, get_score)

# Confusion Matrix
prop.t2 <- (table(test.prop.pred_score, test.supervised$score))
# Accuracy and MAE
cat("Testing Accuracy: ", 
    mean(conv(test.prop.pred_score) == conv(test.supervised$score)), 
    " Testing MAE: ", 
    mean(abs(conv(test.prop.pred_score) - conv(test.supervised$score))))
```

```{r, echo=FALSE}
# plot both confusion matrix side by side for comparison
# align the two tables left and right

# scale the table size
# par(mar = c(4, 4, .1, .1))
# plot the first table
prop.k1 <- kable(prop.t1, caption = "Propodds Training Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)

prop.k2 <- kable(prop.t2, caption = "Propodds Testing Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)

prop.k2
```

Our model does okay, especially given that there are 12 categories. It predicts the correct score only around 32% of the time on the training and 28.7% on the testing data. From the testing confusion matrix, we see that the model is not very good at predicting the lower and upper extremes. It predicts a lot of mid level scores (3 - 4.5), but is not good at differentiating between them. The same results are seen in the training confusion matrix (see Appendix for more). 

This is why we can also look at the MAE, which will give us a better idea of how far off the score predictions are on average, since it is a numerical measure.

On average, the model is off by around 0.55 points on the training and 0.59 points on the testing data. This is a pretty good result, meaning that, on average, the score is only a bit more than one level off.

## \textcolor{magenta!80!black}{K Nearest Neighbors}

During our data analysis/unsupervised learning, we saw that the clustering algorithm(s) did not do a great job of separating the data. This is likely because the data has a lot of overlap. To see if this holds during actual prediction, we can try to use a KNN model to predict the score.

We'll use the `caret` package to tune our `k` value through 10-fold cross validation. A larger `k` will introduce more bias in the model, and a smaller `k` will have a larger variance. Since there are lots of records, a large `k` value could be useful, without introducing too much bias—so we'll test a range of `k` values from 1 to 400. 

```{r, results='hide', echo = FALSE}
set.seed(432)
library(caret)

control <- trainControl(method = "cv", number = 10)
kgrid <- data.frame(k = seq(1, 400, 10))

knn.tuned <- train(score ~ ., data = train.supervised, 
method = "knn", tuneGrid = kgrid, trControl = control)

knn.tuned
```

From the model output, we saw that the best `k` value is 71, with a (cross-validated) accuracy of around 30.66%. We expect to see similar results checking with the entire training dataset. This is similar to our proportional odds model. 

```{r, echo=FALSE}
set.seed(432)
knn.train.pred <- predict(knn.tuned, newdata = train.supervised)
# Confusion Matrix
knn.k1 <- kable(table(knn.train.pred, train.supervised$score), caption = "KNN Training Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)
# Accuracy and MAE
cat("Accuracy: ",
    mean(conv(knn.train.pred) == conv(train.supervised$score)), 
    " MAE: ", mean(abs(conv(knn.train.pred) - conv(train.supervised$score))))
```

Checking the full training data, we see that the accuracy is 34%! This is high, but we should be careful. The model could be overfitting to the training data. 

```{r, echo=FALSE, fig.dim=c(6,3)}
plot(knn.tuned)
```

From the plot, we see the accuracy is low for small `k` values and quickly rises. After that, the accuracy seems to slowly decrease, demonstrating an increased bias (bias-variance tradeoff).

Let's also see how the model does on the testing data.

```{r, echo=FALSE}
set.seed(432)
knn.test.pred <- predict(knn.tuned, newdata = test.supervised)
# Confusion Matrix
knn.k2 <- kable(table(knn.test.pred, test.supervised$score), caption = "KNN Testing Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)
# Accuracy and MAE
cat("Accuracy: ",mean(conv(knn.test.pred) == conv(test.supervised$score)), 
" MAE: ", mean(abs(conv(knn.test.pred) - conv(test.supervised$score))))

knn.k2
```

The model does a noticeably bit worse on the testing data, with an accuracy of around 24.5% (10 percentage points). This is likely an indicator that the model is overfitting to the training data and clusters/predictor spread is different between the training and testing data. This MAE is higher than the proportional odds model (0.57 for training, and 0.66 for testing). The KNN model only looks at euclidean distances. Our unsupervised learning showed that the clusters are overlapping, so it may have a hard time differentiating between scores and can be less accurate than the proportional odds model.

## \textcolor{magenta!80!black}{xgboost}

We saw from the literature review that gradient boosting was effective. So, we'll use xgboost with multi:softmax to predict the score.

We'll tune the `eta` (learning rate), as this is a critical hyperparameter of gradient boosting. For small values of `eta`, the model will take longer to converge, but will be more accurate. For large values of `eta`, the model will converge quickly, but will be less accurate.

We'll use the default `max_depth` (6) and an `nrounds` of 15, as we saw overfitting with larger `nrounds` (50), and will reduce the computational time. We'll pick the model with the lowest training error.

```{r, echo = FALSE}
set.seed(432)
library(xgboost)
eta_vals <- expand.grid(eta = c(0.01, 0.02, 0.03, 0.04, 0.05))
accuracy <- rep(NA, nrow(eta_vals))
for (i in 1:nrow(eta_vals)) {
  model.xgb <- xgboost(data = as.matrix(train.supervised[, -1]), 
  label = as.numeric(train.supervised$score) - 1, 
  nrounds = 15, objective = "multi:softmax", 
  num_class = 12, eta = eta_vals$eta[i], verbose = 0)

  predictions <- predict(model.xgb, as.matrix(train.supervised[, -1]))
  labeled_predictions <- sapply(predictions + 1, score_from_factor)
  accuracy[i] <- mean(conv(labeled_predictions) == conv(train.supervised$score))
}
```

```{r, echo=FALSE}
# eta_vals <- expand.grid(eta = c(0.01, 0.02, 0.03, 0.04, 0.05))
cat("Eta Vals: 0.01, 0.02, 0.03, 0.04, 0.05")
cat("Accuracy per Eta:", accuracy)
best_eta <- eta_vals[which.max(accuracy),]
cat("Best Eta: ",best_eta)
```

We saw that the best `eta` value is `0.05`, with a training accuracy of `0.6024279`. This accuracy is much larger than any other training accuracy we've seen. xgboost can very quickly converge to 100% accuracy on the training data, especially with large `nrounds` and `eta` values. This may be an indicator that the model is overfitting to the training data. We saw this when we used larger `eta` values, so we chose smaller values and we can see how it does on the testing data.

```{r, echo=FALSE}
set.seed(432)
model.xgb <- xgboost(data = as.matrix(train.supervised[, -1]), 
label = as.numeric(train.supervised$score) - 1, nrounds = 15, 
objective = "multi:softmax", 
num_class = 12, eta = best_eta, verbose = 0)

train.predictions <- predict(model.xgb, as.matrix(train.supervised[, -1]))
train.labeled_predictions <- sapply(predictions + 1, score_from_factor)

# confusion matrix
xgb.k1 <- kable(table(train.labeled_predictions, train.supervised$score), caption = "XGBoost Training Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)
# train accuracy and train MAE
cat("Train Accuracy: ", 
    mean(conv(train.labeled_predictions) == conv(train.supervised$score)), 
    " Train MAE: ", 
    mean(abs(conv(labeled_predictions) - conv(train.supervised$score))))
```

We observe a very high training accuracy, for reasons we explained previously. See appendix for the training confusion matrices.

```{r, echo=FALSE}
set.seed(432)
test.predictions <- predict(model.xgb, as.matrix(test.supervised[, -1]))
test.labeled_predictions <- sapply(test.predictions + 1, score_from_factor)

# confusion matrix
xgb.k2 <- kable(table(test.labeled_predictions, test.supervised$score), caption = "XGBoost Testing Confusion Matrix") %>% kable_styling(full_width = F, latex_options = c("scale_down", "HOLD_position"), font_size = 3)

# test accuracy and test mae
cat("Test Accuracy: ", 
    mean(conv(test.labeled_predictions) == conv(test.supervised$score)), 
    "Test MAE: ", 
    mean(abs(conv(test.labeled_predictions) - conv(test.supervised$score))))

xgb.k2
```

Here, we get a testing accuracy of around 30%, which is still lower than the training data (as expected). This is the best test accuracy we've seen so far, as well as an MAE on the lower end (0.61). This makes sense, as gradient boosting is a powerful technique that can capture complex relationships between variables — not just linear relationships or clusters based on euclidean distance. However, all three models have similar accuracy and MAEs for the testing data, so it's hard to definitively say which is the best model—especially when there's randomness involved in the training process.

\newpage
# \textcolor{salmon}{Appendix}

```{r, echo=FALSE}
# Compute silhouette information
silhouette_info <- silhouette(result$cluster, dist(train))

# Define a custom color palette for silhouette plot
custom_palette <- c("#377eb8", "#ff7f00", "#4daf4a", "#f781bf", "#a65628")

plot(silhouette_info, col=custom_palette, border=NA, 
     main="Silhouette Plot of 5 Clusters",
     cex.names=0.8, cex.axis=0.8, cex.main=1, cex.lab=0.9, density=70)

kmeans.test.ct

HC.umap.plot

prop.k1

knn.k1

xgb.k1
```