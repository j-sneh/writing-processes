---
title: "Analysis Fall 23"
author: "Abhi Thanvi, Jonathan Sneh"
date: "2023-12-04"
output: 
  pdf_document: default
toc: yes
header-includes: 
- \usepackage{xcolor}
- \definecolor{salmon}{RGB}{250,150,114}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(2023)
```

```{r echo=FALSE, message=FALSE}
if (!require("stats")) install.packages("stats")
library(stats)
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)
if (!require("tidyr")) install.packages("tidyr")
library(tidyr)
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

if (!require("knitr")) install.packages("knitr")
library(knitr)
if (!require("kableExtra")) install.packages("kableExtra")
library(kableExtra)
if (!require("reshape2")) install.packages("reshape2")
library(reshape2)

if (!require("cluster")) install.packages("cluster")
library(cluster)
if (!require("umap")) install.packages("umap")
library(umap)
if (!require("RColorBrewer")) install.packages("RColorBrewer")
library(RColorBrewer)
```


\newpage
# \textcolor{salmon}{Project Overview} \vspace{4mm}
We highly recommend everyone to checkout our \href{https://github.com/j-sneh/writing-processes}{\textcolor{purple!70!white}{Github Repository}}
 for all the data cleansing, feature engineering, analysis, and other files! Many of our procedures are not included in the report, and the repo provides a behind-the-scenes access to that information! We understand it is important to understand each procedure but also reproduce results, therefore we have maintained a highly intuitive code-base for it! :)

## \textcolor{magenta!80!black}{Goals:}

While this project is an assigned project for STAT 432 Fall 2023 (UIUC), our goal is to apply what we have learned in our class and generate not necessarily the "most accurate", but rather the most holistic solution on this unique problem. The topic we are dealing with \href{https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality}{\textcolor{purple!70!white}{Linking Writing Processes to Writing Quality}} where we explore data on typing behavior to predict essay quality between a score of 0-6 (inclusive) using many of the statistical learning techniques. Our work will help explore the relationship between learners’ writing behaviors and writing performance, which could provide valuable key insights for writing instruction, the development of automated writing evaluation techniques, and help in educational situations.
 

## \textcolor{magenta!80!black}{Approach:} 
We divide our work into three main section. 

  - **Data Processing** $\rightarrow$ Cleanse, extract, and engineer features for our Supervised and Unsupervised learning portion. Our data processing procedure also has some basic EDA work done to allow us to engineer valuable features.
  
  - **Unsupervised Learning** $\rightarrow$ Perform clustering algorithms on the cleansed data (and 80-20 split).
  We chose to do K-Means and Hierarchical Clustering algorithms as we wanted to explore what we learned in class. These unsupervised techniques allow us to find hidden patterns in our data and act as an outlet for advanced EDA. How the clusters were chosen, what insights we drew, the good and bad about these clusters will all be explored later in this section. 
  
  - **Regression/Classification Models** $\rightarrow$ It is in this section where we try to predict scores and aim to achieve our original goal. \textcolor{red}{**JONATHAN ADD YOUR STUFF HERE**} :D

## \textcolor{magenta!80!black}{Unique Approaches/Techniques:} 
We generally tried to use as much as we could from our STAT 432 course materials as the source of knowlegde. There were moments where we did consult other topics such as pairing elbow method with Silhouette Plots (to determine how well the cluster fit), \textcolor{red}{**JONATHAN ADD YOUR STUFF HERE**...prolly the STAT 426 stuff?} :D

## \textcolor{magenta!80!black}{Conclusion:} TBD


\newpage
# \textcolor{salmon}{Literature Review}
Coming soon :'(

\newpage
# \textcolor{salmon}{Data Processing}

This section dives into the tasks performed for data processing. All the steps ensure the specifications of the projects were met, but some decisions were also made to ensure a more practical data to work with. To be considerate of the pages used for the Data Processing, we performed our Data Engineering steps in a `jupyter notebook` that you can view in our repo! 

## \textcolor{magenta!80!black}{Feature Engineering}

- **User ID** [`id`, `string`] — Unique IDs of each user.
  - We keep this to ensure tracking of user information for processing and analysis work.
- **Event ID** [`event_id`, `string`] — Incremental ID log of all events.
  - We keep this for processing steps, but remove it prior to analysis. The event IDs are useful as an ordinal feature of the log data.
- **Down Time / Up Time** [`down_time` / `up_time`, `integer`] — Time of event on down and up strokes of key or button, in seconds.
  - We summarize these features as an array of summary statistics; min, max, mean, median, and standard deviation. Measures of interest are max (i.e., how long a paper is written) and mean/median (i.e., when the center of most activity is).
- **Action Time** [`action_time`, `integer`] — Difference of time between down time and up time of event, i.e., duration of action in seconds. 
  - Similarly, we summarize this feature as min, max, mean, median, and std. This gives insight into "major" consecutive actions, hesitancy, or other special behaviors.
- **Activity** [`activity`, `string`] — Actions to edit or modify the text (input, remove/cut, nonproduction, etc.)
  - We compute the proportions of each of these activities. All of the cursor "Move From" events are mapped to one category called "Move From". We choose proportions over count to avoid undue influence of essays that take longer to write.
- **Down Event**
  - We compute the proportions of each of the activities. The events were pooled into four categories: alphanumeric, special_characters, control_keys, and unknown.
- **Up event**
  - Since these are the same events as down events, we ignore this feature.
- **Text Change**
  - We process and cluster these values into identified patterns of changes: many characters (at least 2 alphanumeric), at least one character (exactly one alphanumeric), non-zero characters (no alphanumeric). We also identified "transition" groups of "X to Y" for each of "many", "single", "none" (e.g., "many" to "many", "many" to "single", "many" to "none", etc.). There was also a "no change" group. We created one additional group to represent the sum of all "transition" events because they coincided exclusively with "replacement" activities.
- **Cursor Position**
  - We computed an artificial array of cursor positions with the assumption that the text was streamed with no edits corresponding to what text changes there are (i.e., non-decreasing and doesn't change if "no change" is observed in text change feature). Then we compute the MAE error metric between this stream version and the actual cursor positions to measure how much error exists between them. Greater errors imply more frequent and/or drastic changes.
- **Word Count**
  - We summarize these features as an array of summary statistics; min, max, mean, median, and standard deviation. We are primarily interested in the maximum measure as it indicates the length of the paper for each user.

## \textcolor{magenta!80!black}{Data Summary}

```{r load data, echo=FALSE}
df = read.csv("data/df_user_summaries_select.csv")
```

Here is a sample of the processed data.

```{r show data, echo=FALSE}
df_show = t(head(df))
colnames(df_show) = df$id[1:6]
# Assuming df is your dataframe
df_show %>% kable("latex", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position"))
```


## \textcolor{magenta!80!black}{Correlation Summary}

```{r plots, echo=FALSE, message=FALSE}
df$mae_cursor_position_norm = df$mae_cursor_position / df$word_count_max
# Assuming 'data' is your dataframe
numeric_features <- df %>% select_if(is.numeric)
# Compute the correlation matrix
cor_matrix <- cor(numeric_features, use = "complete.obs")
# Reshape the data
melted_cor_matrix <- melt(cor_matrix)
# Create the heatmap
ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, 
                       limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_blank()) +coord_fixed()
```

**Discussion**

We observe some pairs of features that show signs of multicolinearity. 

- The word count metrics are highly correlated as expected, we could reasonably choose the maximum measure to use.

- Some features form parallel or perpendicular colinearity. 
  - activity_input and activity_nonproduction (negative)
  - activity_input and down_event_alphanumeric (positive)
  - activity_input and down_event_control_keys (negative)
  - activity_input and text_change_nochange (negative)
  
- Importantly, we're interested in what's correlated with the user score feature
  - word count measures have a positive correlation with score, suggesting an association between longer essays and higher scores
  - Error rate of cursor positions against a "streamed" output also shows a positive correlation with score - i.e., essays written with less frequent or extreme edits is somewhat associated with higher scores.
  - Note: a positive correlation is also found between error rate of cursor positions with max word count, suggesting further that longer essays are associated with higher deviation from a "streamed" output. This suggests the possibility that interpretation of "streamed" deviation is influenced by the paper length (i.e., longer papers support possibility of edits being made "further away" from the current "streamed" position, thus increasing the error rate). When we normalized the error rate by the paper size, we see that the correlation between the normalized error rate and the paper score is nearly zero. So, this feature is likely irrelevant for analysis.

\newpage

# \textcolor{salmon}{Unsupervised Learning Algorithms}

This section dives into the tasks performed for the unsupervised learning algorithms. Currently, focusing on K-Means and Hierarchical Clustering. **THIS SECTION SUPER MESSY RN. Please feel free to edit or improve in any way**

```{r}
# test train split
test_ids = sample(1:nrow(df), as.integer(0.2 * nrow(df)))
data = as.data.frame(scale(df[, -(1:2)]))
data = cbind(score=df$score, data)
train = data[-test_ids, ]
test = data[test_ids, ]
```

## \textcolor{magenta!80!black}{K-Means Algorithm}

using averaged inertia of a few clustering samples across a range of number of clusters (i.e., k=1, ..., 14)

```{r}
# Range of k values to try
cluster_num_list <- 1:14

# Initialize a vector to store average inertias
avg_inertia_list <- numeric(length(cluster_num_list))

# Iterate over different values of k
for (k in cluster_num_list) {
  sub_inertia_list <- numeric(3) # For storing inertia of each trial
  
  for (i in 1:3) {
    set.seed(i) # Setting seed for reproducibility
    kmeans_result <- kmeans(train, centers=k, nstart=25, iter.max = 50)
    sub_inertia_list[i] <- kmeans_result$tot.withinss
  }
  
  avg_inertia_list[k] <- mean(sub_inertia_list)
}

# Plotting the elbow plot
ggplot(data.frame(Clusters=cluster_num_list, Inertia=avg_inertia_list), 
       aes(x=Clusters, y=Inertia)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  ggtitle("Elbow Method for Optimal k") +
  xlab("Number of Clusters") +
  ylab("Average Inertia")

```

selected `k=5` clusters as closest to "elbow" of the inertia plot

```{r}
# Perform K-means clustering
# Here, we are specifying 3 clusters, but you can change this number
result <- kmeans(train, centers=5)

ggplot(data.frame(train), aes(x=score, y=word_count_max)) +
  geom_point(aes(color=factor(result$cluster))) +
  scale_color_discrete(name="Cluster") +
  theme_minimal() +
  ggtitle("K-means Clustering")
```

Using silhouette scores to evaluate how well the clustering structure fits in terms of similarity, i.e., more higher scores in a cluster imply greater similarity of points to its own cluster and poorer similarity to other clusters. The average silhouette score of 0.17 suggests that the clusters are somewhat favorably well separated and that the points within clusters aren't too dispersed. However, cluster 4 shows issues with cohesion and separation.

```{r}
# Compute silhouette information
silhouette_info <- silhouette(result$cluster, dist(train))

# Plotting the silhouette plot
plot(silhouette_info, col=1:k, border=NA, main="Silhouette Plot")
```

Additionally, with projecting the data using UMAP, we see that the clusters might not be very well separated and aren't globular, so it is reasonable to conclude that KMeans algorithm may struggle with this data.

```{r}
umap_result <- umap(train, n_components = 2)

umap_data <- as.data.frame(umap_result$layout)
colnames(umap_data) <- c("UMAP1", "UMAP2")

clusters <- as.factor(result$cluster)
umap_data$Cluster <- clusters

# Choose a palette
palette <- brewer.pal(n = 5, name = "Set1")  # Adjust 'name' as needed

ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = Cluster)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = palette) +
  theme_minimal() +
  ggtitle("UMAP Projection of the Data")

```

Here's distribution of scores in UMAP

```{r}
scores <- as.factor(as.data.frame(train)$score)
umap_data$Score <- scores

ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = Score)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = rainbow(12)) +
  theme_minimal() +
  ggtitle("UMAP Projection of the Data")
```

investigation of the score distribution suggests that the underlying clustering structure of the data does not closely align with the structure of scores.

spread of data between kmeans clusters and scores. characterizations are as follows:

- cluster 1 tends to capture those who score 3 to 6.
- cluster 2 tends to capture those who score between 3.5 to 4.5
- cluster 3 tends to capture those who score 1.5 to 4.5
- cluster 4 tends to capture those who score 3.5 to 4
- cluster 5 tends to capture those who score 2.5 to 4.5

e.g., a user who scores around 5 is likely to be in cluster 1

while there are some relationships, it would appear that the dispersions of scores between groups tend to overlap heavily and are not well separate to segment the groups in a very meaningful way. However clusters 1 and 2 vs. clusters 3, 4, and 5 seem to show some disparity.

```{r}
t(table(result$cluster, df[-test_ids, ]$score)) / nrow(df[-test_ids, ])
```

to predict on new data in test...

```{r}
assign_cluster <- function(new_data, centers) {
  # Calculate Euclidean distances from each new data point to each cluster center
  distances <- as.matrix(dist(rbind(centers, new_data), method = "euclidean"))
  distances <- distances[(nrow(centers)+1):nrow(distances), 1:nrow(centers)]
  
  # Assign each new data point to the nearest cluster
  max.col(-distances)
}

new_clusters <- assign_cluster(test, result$centers)
```

```{r}
acc_data = as.matrix(
  t(table(new_clusters, df[test_ids, ]$score)) / nrow(df[test_ids, ])
)

for (i in 1:5) {
  prop_plot = ggplot(
  data.frame(
      Index = rownames(acc_data), Proportion = acc_data[, i]
    ), 
  aes(x = Index, y = Proportion)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  xlab("Index") +
  ylab("Proportion") +
  ggtitle("Histogram of Proportions") + 
  ylim(c(0, 1))
  print(prop_plot)
}
```


```{r}
acc_data = as.matrix(
  t(table(new_clusters, as.integer(df[test_ids, ]$score))) / nrow(df[test_ids, ])
)
```

```{r}
for (i in 1:5) {
  prop_plot = ggplot(
  data.frame(
      Index = rownames(acc_data), Proportion = acc_data[, i]
    ), 
  aes(x = Index, y = Proportion)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  xlab("Index") +
  ylab("Proportion") +
  ggtitle("Histogram of Proportions") + 
  ylim(c(0, 1))
  print(prop_plot)
}

```

selected Ward's linkage due to cohesion and separability issues in the data. we select 7 clusters because there are 7 integer scores (after rounding half scores). The clustering structure in the data seems to support this number of clusters.

## \textcolor{magenta!80!black}{Hierarchial Clustering}

```{r}
distance_matrix <- dist(train, method = "euclidean")

hc <- hclust(distance_matrix, method = "ward.D")

plot(hc, main = "Hierarchical Clustering with Complete Linkage", 
     xlab = "", sub = "", cex = 0.9)
```

```{r}
k <- 7  # Number of clusters
plot(hc, main = "Hierarchical Clustering with Complete Linkage", 
     xlab = "", sub = "", cex = 0.9)
rect.hclust(hc, k = k, border = "red")  # You can change the border color if you like
```

clustering structure in UMAP

```{r}
clusters <- as.factor(cutree(hc, k = k))
umap_data$ClusterHC <- clusters

# Choose a palette
palette <- brewer.pal(n = k, name = "Set1")  # Adjust 'name' as needed

ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = ClusterHC)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = palette) +
  theme_minimal() +
  ggtitle("UMAP Projection of the Data")
```

investigation of the score distribution suggests that the underlying clustering structure of the data ...

spread of data between kmeans clusters and scores. characterizations are as follows:

- cluster 1 tends to capture those who score 2.5 to 4.5
- cluster 2 tends to capture those who score 3.5 to 4.5
- cluster 3 tends to capture those who score 1.5 to 4.5
- cluster 4 tends to capture those who score 2.5 to 4
- cluster 5 tends to capture those who score 3.5 to 6
- cluster 6 tends to capture those who score 2.5 to 4.5
- cluster 7 tends to capture those who score 3 to 4.5

e.g., a user who scores around 5 is likely to be in cluster 5


```{r}
t(table(umap_data$ClusterHC, df[-test_ids, ]$score)) / nrow(df[-test_ids, ])
```

to predict on new data in test...

```{r}
# Function to calculate centroids of clusters
calculate_centroids <- function(data, clusters) {
  aggregate(data, by=list(cluster=clusters), FUN=mean)
}

# Function to predict the cluster of new data
predict_cluster <- function(new_data, train_data, clusters) {
  centroids <- calculate_centroids(train_data, clusters)
  # Remove the cluster column
  centroids <- centroids[, -1]
  
  # Function to find nearest centroid
  find_nearest_centroid <- function(point, centroids) {
    dists <- apply(centroids, 1, function(centroid) dist(rbind(centroid, point)))
    which.min(dists)
  }
  
  apply(new_data, 1, find_nearest_centroid, centroids = centroids)
}

new_clusters <- predict_cluster(test, train, clusters)
```

```{r}
acc_data = as.matrix(
  t(table(new_clusters, as.integer(df[test_ids, ]$score))) / nrow(df[test_ids, ])
)
```

```{r}
for (i in 1:7) {
  prop_plot = ggplot(
  data.frame(
      Index = rownames(acc_data), Proportion = acc_data[, i]
    ), 
  aes(x = Index, y = Proportion)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  xlab("Index") +
  ylab("Proportion") +
  ggtitle("Histogram of Proportions") + 
  ylim(c(0, 1))
  print(prop_plot)
}

```


# \textcolor{salmon}{Supervised Learning Algorithms}

## \textcolor{magenta!80!black}{Proportional Odds Model (GLM)}

Since the score is an ordinal variable, we can use a proportional odds model to predict the score. We'll use the `VGAM` package to fit the model and use the `step4` function to do a stepwise selection to find the best model. `VGAM` is a package that allows for fitting of a multinomial/propodds (vector based) linear model. It assumes cumulative probabilities and keeps the same $\beta$ for all categories, but allows for different intercepts. 

We'll use only non-collinear features and do a subset selection. We'll also convert `score` to an ordered factor to ensure that the model knows that it is an ordinal variable.

```{r}
library(VGAM)
train.propodds <- train 
train.propodds$score <- as.ordered(train.propodds$score)
train.propodds <- subset(train.propodds, select=c(score, word_count_max, down_event_special_character, mae_cursor_position, down_time_std, down_event_control_keys, text_change_not_q, activity_input))

test.propodds <- test
test.propodds$score <- as.ordered(test.propodds$score)
test.propodds <- subset(test.propodds, select=c(score, word_count_max, down_event_special_character, mae_cursor_position, down_time_std, down_event_control_keys, text_change_not_q, activity_input))

head(train.propodds)
```

```{r}
prop.wc <- vglm(score ~ word_count_max, data = train.propodds, family = propodds(reverse = F))

prop.upper <- vglm(score ~ ., data = train.propodds, family = propodds(reverse = F))

summary(prop.upper)
```

We can do a stepwise selection, starting from all variables, to find the best model. This will pick the model with the lowest AIC, which aims for better prediction error. 

```{r}
prop.step <- step4(prop.upper, scope = list(lower = prop.wc, upper = prop.upper), direction = "both")

summary(prop.step)
```

Using our selected model, we can predict the probabilities of falling into each category and selecting the category with the highest probability as the predicted score. We can then compare the predicted scores to the actual scores to see how well our model did.

```{r}
head(predict(prop.step, newdata = train.propodds, type = "response"))

train.prop.probs <- predict(prop.step, newdata = train.propodds, type = "response")

labels <- sort(unique(train.propodds$score))
get_score <- function(x) { labels[which.max(x)] }
# Column with maximum probability is the predicted score (label)
train.prop.pred_score <- apply(train.prop.probs, 1, get_score)

# Compare predicted scores to actual scores
table(train.prop.pred_score, train.propodds$score)
mean(as.numeric(train.prop.pred_score) == as.numeric(train.propodds$score))
```

```{r}
test.prop.probs <- predict(prop.step, newdata = test.propodds, type = "response")

# Column with maximum probability is the predicted score (label)
test.prop.pred_score <- apply(test.prop.probs, 1, get_score)

# Compare predicted scores to actual scores
table(test.prop.pred_score, test.propodds$score)
mean(as.numeric(test.prop.pred_score) == as.numeric(test.propodds$score))
```

Unfortunately, our model does not do very well. It predicts the correct score only around one third of the time on both the training and testing data. From the confusion matrix, we see that the model is not very good at predicting the lower scores. It predicts a lot of the mid level scores (3 - 4.5), but is not good at differentiating between them. 

Instead of looking at the predicted classification accuracy, we can look at the MSE. This will give us a better idea of how far off the predictions are. 

```{r}
mean(abs(as.numeric(train.prop.pred_score) - as.numeric(train.propodds$score)))
mean(abs(as.numeric(test.prop.pred_score) - as.numeric(test.propodds$score)))
```
On average, the model is off by around 1.1-1.2 points on both the training and testing data. This is still a pretty large difference considering that the scores range from 1 to 6, but not horrible.